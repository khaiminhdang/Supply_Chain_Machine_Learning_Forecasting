# Norway New Car Sales Forecasting â€“ Machine Learning Benchmark

**Multi-model time series forecasting comparison** on monthly new passenger car registrations by make in Norway.

<img src="https://img.shields.io/badge/Python-3.8%2B-blue"> <img src="https://img.shields.io/badge/scikit--learn-1.0+-orange"> <img src="https://img.shields.io/badge/XGBoost-1.7+-yellow"> <img src="https://img.shields.io/badge/Pandas-2.0+-green">

## ðŸŽ¯ Objective

Build, compare and progressively improve forecasting models to predict future monthly new car sales per brand using historical data (2007â€“present).

The project systematically moves from simple autoregressive approaches to advanced ensemble and neural methods, while exploring:

- Sliding-window time-series preparation
- Exogenous variables (GDP)
- Categorical features (brand segment, one-hot encoding)
- Seasonal pattern clustering
- Feature selection / importance analysis
- Multi-step forecasting

## ðŸ“Š Dataset

- **File**: `norway_new_car_sales_by_make.csv`
- **Source**: originally from Kaggle â†’ [New Car Sales in Norway](https://www.kaggle.com/datasets/dmi3kno/newcarsalesnorway) (also mirrored in many public repos)
- **Structure**: wide format â†’ rows = car makes, columns = YYYY-MM periods, values = registered new passenger cars
- **Time range**: January 2007 â†’ last available month
- **Exogenous data**: yearly GDP (used as simple macro indicator)

## ðŸ›  Models Implemented & Compared

| Stage | Model Family                  | Key Techniques / Variants                                 | Notes                                      |
|------|-------------------------------|-------------------------------------------------------------------|--------------------------------------------|
| 2.3  | Linear Regression             | AR(12) autoregressive                                             | Baseline                                   |
| 2.4  | Decision Tree                 | max_depth=5, min_samples_split=15, criterion MSE vs MAE           | Tree visualisation                         |
| 2.5  | Tuned Decision Tree           | RandomizedSearchCV (100 trials, 10-fold CV)                       | Hyperparameter tuning                      |
| 2.6  | Random Forest                 | n_estimators=30 â†’ 200, tuned with 400 trials                      | Feature importance plot                    |
| 2.8  | Extra Trees (Extremely Randomized Trees) | n_estimators=200, tuned                                     | Higher randomness than RF                  |
| 2.9  | Feature-length optimization   | Test x_len = 6..50 months with RF & ExtraTrees                    | Optimal lag length analysis                |
| 2.10 | AdaBoost                      | DecisionTree base, tuned learning_rate & loss                     | Boosting approach                          |
| 2.12 | XGBoost                       | Single-step & multi-step (MultiOutputRegressor), early stopping, tuning with 1000 trials | Strongest single model in most runs        |
| 2.13 | Categorical encoding          | Integer encoding (segment) + one-hot (brand)                      | Brand-specific effects                     |
| 2.14 | Seasonal clustering           | KMeans on normalised monthly seasonal factors                     | Group brands by seasonality pattern        |
| 2.15 | Rich feature set + XGBoost    | Aggregated statistics + month + GDP + segment/one-hot + clustering | Feature importance driven selection        |
| 2.16 | Neural Network (MLPRegressor) | Adam, early stopping, RandomizedSearchCV on architecture & reg   | Final deep learning baseline               |

## ðŸ“ˆ Main Results (typical ranges from notebook runs)

| Model                     | Test MAE% | Test RMSE% | Bias% | Training time | Comment                              |
|---------------------------|-----------|------------|-------|---------------|--------------------------------------|
| Linear Regression         | ~38â€“45%   | ~60â€“80%    | ~Â±5%  | <1s           | Surprisingly strong baseline         |
| Decision Tree             | 32â€“42%    | 55â€“75%     | low   | ~0.5s         | Overfits easily                      |
| Tuned Tree                | ~30â€“38%   | ~50â€“68%    | low   | ~2â€“5s         | Clear improvement                    |
| Random Forest (200)       | 24â€“32%    | 42â€“58%     | ~0â€“3% | ~5â€“15s        | Very stable                          |
| ExtraTrees (200)          | 23â€“31%    | 40â€“56%     | low   | ~4â€“12s        | Slightly better than RF in some runs |
| XGBoost (tuned)           | **21â€“28%**| **36â€“50%** | ~0â€“2% | ~3â€“20s        | Best single-model performance        |
| XGBoost + rich features   | **19â€“26%**| **33â€“47%** | low   | ~5â€“30s        | Best overall result in notebook      |
| MLP (tuned)               | 27â€“38%    | 48â€“70%     | â€”     | ~10â€“120s      | Requires more tuning & data          |

*(exact numbers depend on train/test split date & random seed)*

## ðŸ“ Repository Structure (suggested)
